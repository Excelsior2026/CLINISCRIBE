# CliniScribe Environment Configuration
# Copy this file to .env and customize for your setup

# =============================================================================
# MODEL SETTINGS
# =============================================================================

# Whisper model size: tiny, base, small, medium, large-v3
# Smaller = faster, larger = more accurate
# Recommended for students: base or small
WHISPER_MODEL=base

# Use GPU for faster transcription (requires CUDA)
# Set to true if you have an NVIDIA GPU with CUDA installed
USE_GPU=false

# Ollama model for summarization
# Options: llama3.1:8b, llama3.1:70b, llama2:13b, etc.
OLLAMA_MODEL=llama3.1:8b

# Ollama server connection
OLLAMA_HOST=localhost
OLLAMA_PORT=11434

# Timeout for summarization in seconds (increase for long transcripts)
OLLAMA_TIMEOUT=300

# =============================================================================
# FILE UPLOAD SETTINGS
# =============================================================================

# Maximum file size in megabytes
# 500MB is suitable for ~8 hour recordings at reasonable quality
MAX_FILE_SIZE_MB=500

# Audio storage directory (where uploaded files are saved)
AUDIO_STORAGE_DIR=audio_storage

# Temporary processing directory (for preprocessed audio)
TEMP_AUDIO_DIR=temp_processed

# Days to keep uploaded audio files before automatic cleanup
# Set to 0 to disable automatic cleanup
AUDIO_RETENTION_DAYS=7

# =============================================================================
# LOGGING SETTINGS
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR
# Use DEBUG for troubleshooting, INFO for normal operation
LOG_LEVEL=INFO

# =============================================================================
# EXAMPLE CONFIGURATIONS FOR DIFFERENT USE CASES
# =============================================================================

# For laptops with limited resources:
# WHISPER_MODEL=tiny
# OLLAMA_MODEL=llama3.1:8b
# MAX_FILE_SIZE_MB=200
# USE_GPU=false

# For workstations with powerful GPUs:
# WHISPER_MODEL=large-v3
# OLLAMA_MODEL=llama3.1:70b
# MAX_FILE_SIZE_MB=1000
# USE_GPU=true

# For quick testing:
# WHISPER_MODEL=tiny
# OLLAMA_MODEL=llama3.1:8b
# OLLAMA_TIMEOUT=60
# LOG_LEVEL=DEBUG
