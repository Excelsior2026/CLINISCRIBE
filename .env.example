# CliniScribe Configuration
# Copy this file to .env and adjust values as needed

# =============================================================================
# WHISPER MODEL SETTINGS
# =============================================================================
# Available models: tiny, base, small, medium, large-v3
# Smaller models = faster but less accurate
# Larger models = more accurate but slower and need more RAM
WHISPER_MODEL=base

# Enable GPU acceleration (requires CUDA-capable GPU)
# Set to 'true', '1', or 'yes' to enable
USE_GPU=false

# =============================================================================
# OLLAMA SETTINGS
# =============================================================================
# Ollama server connection
OLLAMA_HOST=localhost
OLLAMA_PORT=11434

# Model for summarization (must be pulled with: ollama pull <model>)
# Recommended: llama3.1:8b (balanced), llama3.1:70b (high quality)
OLLAMA_MODEL=llama3.1:8b

# Timeout for summarization requests (seconds)
# Increase for longer transcripts or slower hardware
OLLAMA_TIMEOUT=300

# =============================================================================
# FILE UPLOAD SETTINGS
# =============================================================================
# Maximum file size for uploads (in MB)
MAX_FILE_SIZE_MB=500

# How many days to keep processed audio files before cleanup
AUDIO_RETENTION_DAYS=7

# Storage directories
AUDIO_STORAGE_DIR=audio_storage
TEMP_AUDIO_DIR=temp_processed

# =============================================================================
# LOGGING
# =============================================================================
# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# =============================================================================
# EXAMPLE CONFIGURATIONS
# =============================================================================

# ---- Configuration 1: Laptop/Desktop (Fast, Good Quality) ----
# WHISPER_MODEL=small
# USE_GPU=false
# OLLAMA_MODEL=llama3.1:8b
# MAX_FILE_SIZE_MB=200

# ---- Configuration 2: Workstation with GPU (Best Quality) ----
# WHISPER_MODEL=large-v3
# USE_GPU=true
# OLLAMA_MODEL=llama3.1:70b
# MAX_FILE_SIZE_MB=1000
# OLLAMA_TIMEOUT=600

# ---- Configuration 3: Low-Resource Device (Maximum Speed) ----
# WHISPER_MODEL=tiny
# USE_GPU=false
# OLLAMA_MODEL=llama3.1:8b
# MAX_FILE_SIZE_MB=100
# OLLAMA_TIMEOUT=180
